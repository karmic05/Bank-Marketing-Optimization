{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2NjL-wzsdOh"
      },
      "source": [
        "# Bank Marketing: Term Deposit Prediction\n",
        "## A2: Classification Case Study\n",
        "\n",
        "**Author:** [Your Name]  \n",
        "**Date:** February 18, 2026  \n",
        "**Objective:** Predict whether a client will subscribe to a term deposit (Target: `y`).\n",
        "\n",
        "### Business Context\n",
        "The bank wants to optimize its direct marketing campaigns. Telemarketing is expensive; calling customers who are unlikely to buy is a waste of resources. By predicting the propensity to subscribe (`y=1`), the bank can focus its effort on high-probability leads, increasing conversion rates and reducing costs.\n",
        "\n",
        "### Analytical Approach\n",
        "1.  **Data Preparation:** Handling categorical variables and the binary target.\n",
        "2.  **Feature Engineering:** One-Hot Encoding for categorical data.\n",
        "3.  **Modeling:** Testing candidate models (Logistic Regression, Random Forest, GBM) focusing on **AUC** due to class imbalance.\n",
        "4.  **Evaluation:** Analyzing the **Train-Test Gap** to ensure the model is not overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUR_ZneJsdOj"
      },
      "source": [
        "## 1. Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xB1jMR2HsdOk"
      },
      "outputs": [],
      "source": [
        "# Importing essential libraries\n",
        "import pandas as pd                       # Data manipulation\n",
        "import numpy as np                        # Mathematical operations\n",
        "import matplotlib.pyplot as plt           # Base plotting\n",
        "import seaborn as sns                     # Enhanced plotting\n",
        "\n",
        "# Machine Learning Utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, classification_report, make_scorer\n",
        "\n",
        "# Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Suppress warnings for clean output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Pandas display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "print(\"Libraries imported successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6xZURiYsdOl"
      },
      "source": [
        "## 2. Data Loading and Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6Daq0QJsdOl"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "# Note: The UCI Bank Marketing dataset uses a semicolon ';' delimiter\n",
        "try:\n",
        "    df = pd.read_csv('bank-full.csv', sep=';')\n",
        "    print(\"Data loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'bank-full.csv' not found. Please ensure the file is uploaded.\")\n",
        "\n",
        "# Map target 'y' to binary (1 = yes, 0 = no)\n",
        "df['y_binary'] = df['y'].map({'yes': 1, 'no': 0})\n",
        "\n",
        "# DROP VARIABLES:\n",
        "# 1. 'y': Original target (replaced by y_binary)\n",
        "# 2. 'poutcome': This variable often causes data leakage (predicting the future based on the future)\n",
        "df = df.drop(['y', 'poutcome'], axis=1)\n",
        "\n",
        "# Check Class Imbalance\n",
        "print(f\"\\nDataset Shape: {df.shape}\")\n",
        "print(\"Target Variable Distribution:\")\n",
        "print(df['y_binary'].value_counts(normalize=True).round(4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xYTi0K8sdOl"
      },
      "source": [
        "## 3. Feature Engineering (One-Hot Encoding)\n",
        "We convert categorical variables (text) into numerical dummy variables so the algorithms can process them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nT7XDyIYsdOl"
      },
      "outputs": [],
      "source": [
        "# Identify column types\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Create dummy variables\n",
        "# drop_first=True avoids the \"dummy variable trap\" (multicollinearity) for linear models\n",
        "df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "print(f\"Shape after One-Hot Encoding: {df_encoded.shape}\")\n",
        "df_encoded.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gjC9rCLsdOm"
      },
      "source": [
        "## 4. Train-Test Split\n",
        "We use **Stratified Sampling** because the data is imbalanced (88% No / 12% Yes). This ensures the Train and Test sets have the same proportion of \"Yes\" customers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHXDhix7sdOm"
      },
      "outputs": [],
      "source": [
        "# 1. Separate Features (x) and Target (y)\n",
        "y_target = df_encoded['y_binary']\n",
        "x_features = df_encoded.drop('y_binary', axis=1)\n",
        "\n",
        "# 2. Split Data\n",
        "# Random State 219 is used for reproducibility\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x_features,\n",
        "    y_target,\n",
        "    test_size=0.25,\n",
        "    random_state=219,\n",
        "    stratify=y_target  # CRITICAL for imbalanced data\n",
        ")\n",
        "\n",
        "print(f\"Training Data: {x_train.shape}\")\n",
        "print(f\"Testing Data:  {x_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7QynzHSsdOm"
      },
      "source": [
        "## 5. Model Candidate Loop\n",
        "We test multiple models to find the best performer.\n",
        "\n",
        "**Metric Focus:**\n",
        "* **AUC (Area Under Curve):** The primary metric because accuracy is misleading on imbalanced data.\n",
        "* **Train-Test Gap:** We calculate the difference between Training Score and Testing Score. A large gap (> 0.05) indicates **Overfitting**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KKLYuWksdOm"
      },
      "outputs": [],
      "source": [
        "# Dictionary of models to test\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "    'Decision Tree':       DecisionTreeClassifier(random_state=42, max_depth=8),\n",
        "    'Random Forest':       RandomForestClassifier(random_state=42, n_estimators=100, max_depth=8),\n",
        "    'Gradient Boosting':   GradientBoostingClassifier(random_state=42, learning_rate=0.1, max_depth=3)\n",
        "}\n",
        "\n",
        "# DataFrame to store results\n",
        "model_results = []\n",
        "\n",
        "print(\"Training models...\\n\")\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Create a pipeline to scale data automatically\n",
        "    # Scaling is required for Logistic Regression, but optional for Trees\n",
        "    # We apply it to all for consistency in this loop\n",
        "    pipe = Pipeline([('scaler', StandardScaler()), ('clf', model)])\n",
        "\n",
        "    # Fit the model\n",
        "    pipe.fit(x_train, y_train)\n",
        "\n",
        "    # Predict Probabilities (needed for AUC)\n",
        "    # [:, 1] grabs the probability of the positive class (1)\n",
        "    y_train_pred_proba = pipe.predict_proba(x_train)[:, 1]\n",
        "    y_test_pred_proba  = pipe.predict_proba(x_test)[:, 1]\n",
        "\n",
        "    # Calculate Scores (AUC)\n",
        "    train_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
        "    test_auc  = roc_auc_score(y_test, y_test_pred_proba)\n",
        "\n",
        "    # Calculate Gap\n",
        "    gap = abs(train_auc - test_auc)\n",
        "\n",
        "    # Store results\n",
        "    model_results.append({\n",
        "        'Model': name,\n",
        "        'Train AUC': train_auc,\n",
        "        'Test AUC': test_auc,\n",
        "        'Gap': gap\n",
        "    })\n",
        "\n",
        "    print(f\"{name} processed.\")\n",
        "\n",
        "# Create a results DataFrame and sort by Test AUC\n",
        "results_df = pd.DataFrame(model_results)\n",
        "results_df = results_df.sort_values(by='Test AUC', ascending=False)\n",
        "\n",
        "print(\"\\n--- Model Performance Metrics ---\")\n",
        "results_df.round(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueROgWkXsdOm"
      },
      "source": [
        "## 6. Final Model Selection\n",
        "Based on the results above, we select the model with the highest **Test AUC** that also maintains a reasonable **Gap** (indicating stability)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1rOUfxXsdOm"
      },
      "outputs": [],
      "source": [
        "# Select best model name dynamically\n",
        "best_model_name = results_df.iloc[0]['Model']\n",
        "print(f\"The best performing model is: {best_model_name}\")\n",
        "\n",
        "# Retrain the best model for final visualization (Confusion Matrix)\n",
        "best_model = models[best_model_name]\n",
        "final_pipe = Pipeline([('scaler', StandardScaler()), ('clf', best_model)])\n",
        "final_pipe.fit(x_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = final_pipe.predict(x_test)\n",
        "\n",
        "# Confusion Matrix Visualization\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Predicted No', 'Predicted Yes'],\n",
        "            yticklabels=['Actual No', 'Actual Yes'])\n",
        "plt.title(f'Confusion Matrix: {best_model_name}')\n",
        "plt.xlabel('Prediction')\n",
        "plt.ylabel('Truth')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2VIA4kRsdOn"
      },
      "source": [
        "## 7. Conclusion\n",
        "\n",
        "1.  **Model Choice:** The **Gradient Boosting Classifier** (or whichever appears top) provided the best balance of AUC and stability.\n",
        "2.  **Implication:** By using this model, the bank can sort customers by \"Probability to Subscribe.\"\n",
        "3.  **Next Steps:** The marketing team should focus strictly on the top decile of customers identified by this model to maximize ROI."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}